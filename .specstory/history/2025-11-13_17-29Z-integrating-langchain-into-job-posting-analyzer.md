<!-- Generated by SpecStory, Markdown v2.1.0 -->

<!-- vscode Session 3ce84419-a37f-417f-9d02-2d4479137c6c (2025-11-13 17:29Z) -->

# Integrating LangChain into Job Posting Analyzer (2025-11-13 17:29Z)

_**User (2025-11-13 17:31Z)**_

What are some ways that LangChain could be useful for this project?

---

_**Agent (model copilot/grok-code-fast-1)**_

Based on the architecture of your job-posting-analyzer project, LangChain could significantly enhance several aspects of your AI-powered resume analysis system. Here are the most relevant ways it could be useful:

## 1. **Advanced Document Processing & Parsing**

Your current system handles PDF, text, and markdown files for resumes and job postings. LangChain offers robust document loaders that could improve:

- **Better PDF parsing** with `PyPDFLoader` or `PDFMinerLoader` for more accurate text extraction
- **Structured data extraction** from resumes using LangChain's document transformers
- **Multi-format support** for additional file types (DOCX, HTML job postings, etc.)

```typescript
// Example: Enhanced document loading
import { DirectoryLoader } from "langchain/document_loaders/fs/directory";
import { PDFLoader } from "langchain/document_loaders/fs/pdf";

const loader = new DirectoryLoader("./uploads", {
  ".pdf": (path) => new PDFLoader(path),
  ".txt": (path) => new TextLoader(path),
});
```

## 2. **Retrieval-Augmented Generation (RAG) for Better Analysis**

Your current analysis compares resumes against job postings. LangChain's RAG capabilities could:

- **Create searchable knowledge bases** from resume content and job requirements
- **Improve skill matching** by retrieving relevant experience sections from resumes
- **Context-aware gap analysis** by finding similar past experiences
- **Dynamic prompt enhancement** using retrieved context

```typescript
// Example: RAG-enhanced skill matching
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const vectorStore = await MemoryVectorStore.fromDocuments(
  resumeDocuments, 
  new OpenAIEmbeddings()
);

const relevantSkills = await vectorStore.similaritySearch(
  jobPosting.requirements, 
  5
);
```

## 3. **Sophisticated Analysis Chains**

Your current `LLMServiceInterface` does direct API calls. LangChain chains could create multi-step analysis workflows:

- **Sequential analysis pipeline**: Extract skills ‚Üí Match against resume ‚Üí Identify gaps ‚Üí Generate suggestions ‚Üí Create cover letter
- **Conditional logic**: Different analysis paths based on job type or resume completeness
- **Error handling and retries** built into the chain structure

```typescript
// Example: Multi-step analysis chain
import { LLMChain, SequentialChain } from "langchain/chains";

const skillExtractionChain = new LLMChain({
  prompt: skillExtractionPrompt,
  llm: yourLLMService,
});

const gapAnalysisChain = new LLMChain({
  prompt: gapAnalysisPrompt, 
  llm: yourLLMService,
});

const analysisChain = new SequentialChain({
  chains: [skillExtractionChain, gapAnalysisChain],
  inputVariables: ["resume", "jobPosting"],
  outputVariables: ["skills", "gaps", "suggestions"],
});
```

## 4. **Enhanced Memory Management**

Your project has conversation history features. LangChain's memory systems could:

- **Persistent conversation context** across analysis sessions
- **Resume-specific memory** that remembers past analyses for the same resume
- **Job-specific insights** that accumulate knowledge about similar job types
- **Better cover letter generation** with memory of previous successful letters

```typescript
// Example: Conversation memory for iterative analysis
import { ConversationBufferMemory } from "langchain/memory";

const memory = new ConversationBufferMemory({
  memoryKey: "analysis_history",
  returnMessages: true,
});

const chain = new ConversationChain({
  llm: yourLLMService,
  memory: memory,
  prompt: analysisPrompt,
});
```

## 5. **Tool Integration for Richer Analysis**

LangChain's tool system could extend your analysis capabilities:

- **External data sources**: Job market trends, salary data, skill demand
- **LinkedIn integration**: Company information, job posting validation
- **Skill databases**: Standardized skill taxonomies for better matching
- **Calendar integration**: Interview scheduling suggestions

```typescript
// Example: Tool-augmented analysis
import { Tool } from "langchain/tools";

class JobMarketTool extends Tool {
  name = "job_market_research";
  
  async _call(query: string): Promise<string> {
    // Research current job market trends
    return await researchJobMarket(query);
  }
}

const tools = [new JobMarketTool()];
const agent = initializeAgent(tools, yourLLMService);
```

## 6. **Prompt Engineering & Management**

Your current system likely has hardcoded prompts. LangChain could provide:

- **Dynamic prompt templates** that adapt based on job type or resume quality
- **Few-shot learning** examples for better analysis quality
- **Prompt versioning** and A/B testing
- **Multi-language support** for international job markets

```typescript
// Example: Dynamic prompt management
import { PromptTemplate } from "langchain/prompts";

const analysisTemplate = new PromptTemplate({
  template: `
Analyze this {jobType} position against the candidate's resume.

Job Posting: {jobPosting}
Resume: {resume}

Focus areas for {jobType} roles:
{specializedInstructions}

Provide analysis in the following format:
{outputFormat}
  `,
  inputVariables: ["jobType", "jobPosting", "resume", "specializedInstructions", "outputFormat"],
});
```

## 7. **Agent-Based Analysis Workflows**

For complex analysis scenarios, LangChain agents could:

- **Autonomous analysis planning**: Decide what analysis steps to take based on job complexity
- **Iterative refinement**: Ask follow-up questions or perform additional research
- **Multi-provider orchestration**: Use different AI models for different analysis aspects
- **Quality assurance**: Self-review and correction of analysis results

## 8. **Integration with Your Current Architecture**

LangChain could complement your existing service layer:

```typescript
// Enhanced LLMServiceInterface with LangChain
class LangChainLLMService extends BaseLLMService {
  private vectorStore: VectorStore;
  private analysisChain: Chain;
  
  async analyzeJobPosting(jobPosting: JobPosting, resume: Resume): Promise<AnalysisResult> {
    // Use LangChain for sophisticated analysis
    const docs = await this.loadDocuments(resume);
    await this.vectorStore.addDocuments(docs);
    
    return await this.analysisChain.call({
      jobPosting: jobPosting.content,
      resumeContext: await this.retrieveRelevantResumeSections(jobPosting),
    });
  }
}
```

## Implementation Considerations

**Pros:**
- Significantly more sophisticated analysis capabilities
- Better handling of complex document formats
- Improved accuracy through RAG and chaining
- Extensible architecture for future enhancements

**Cons:**
- Additional complexity and dependencies
- Learning curve for LangChain concepts
- Potential performance overhead
- May require rethinking current service architecture

**Migration Path:**
1. Start with document loading improvements
2. Add RAG for better context retrieval
3. Implement chains for multi-step analysis
4. Gradually migrate analysis logic to LangChain patterns

Would you like me to explore any of these approaches in more detail or help you implement a specific LangChain integration?

---

_**User (2025-11-18 22:03Z)**_

Write this up as a markdown file in `docs/plans/`

---

_**Agent (model copilot/oswe-vscode-prime)**_

<tool-use data-tool-type="task" data-tool-name="manage_todo_list">
<details>
<summary>Tool use: **manage_todo_list** ‚Ä¢ Using "Manage and track todo items for task planning"</summary>

**Created 3 todos**

- [ ] Draft plan
  > Create a plan file in docs/plans/ describing LangChain integration, with scope, approach, steps, testing, and risks
- [ ] Create Markdown file
  > Write the actual docs/plans/13.langchain-integration.md file with full content
- [ ] Review and finalize
  > Review the plan file for completeness, follow project plan structure and mark as completed

</details>
</tool-use>

---

<tool-use data-tool-type="task" data-tool-name="manage_todo_list">
<details>
<summary>Tool use: **manage_todo_list** ‚Ä¢ Using "Manage and track todo items for task planning"</summary>

**Starting: *Draft plan* (1/3)**

- [-] Draft plan
  > Create a plan file in docs/plans/ describing LangChain integration, with scope, approach, steps, testing, and risks
- [ ] Create Markdown file
  > Write the actual docs/plans/13.langchain-integration.md file with full content
- [ ] Review and finalize
  > Review the plan file for completeness, follow project plan structure and mark as completed

</details>
</tool-use>

---

<tool-use data-tool-type="unknown" data-tool-name="copilot_createFile">
<details>
<summary>Tool use: **copilot_createFile**</summary>


Parameters:

```json
{
  "file_path": "/Users/michaelpaulukonis/projects/job-posting-analyzer/docs/plans/13.langchain-integration.md",
  "relativeWorkspacePath": "13.langchain-integration.md"
}
```
Additional data:

```json
{
  "vscodeToolId": "copilot_createFile",
  "isHidden": false
}
```
Result:

```json
{
  "invocationMessage": "Creating [](file:///Users/michaelpaulukonis/projects/job-posting-analyzer/docs/plans/13.langchain-integration.md)",
  "pastTenseMessage": "Created [](file:///Users/michaelpaulukonis/projects/job-posting-analyzer/docs/plans/13.langchain-integration.md)",
  "isConfirmed": {
    "type": 1
  },
  "content": "",
  "textEditContent": "# 13. LangChain Integration Plan\n\n## Overview\n\nThis plan describes how LangChain can be integrated into the Job Posting Analyzer project to improve document processing, contextual retrieval, analysis reliability, and developer productivity. The objective is to leverage LangChain's chains, memory, RAG (Retrieval-Augmented Generation), and tool/agent abstractions to build more robust, maintainable, and extensible analysis pipelines without changing the existing UI or user experience.\n\n## Problem Statement\n\nThe current system uses a service-based LLM integration with provider-specific classes (e.g., &#96;GeminiLLMService&#96;, &#96;ClaudeLLMService&#96;, &#96;MockLLMService&#96;). While functional, the system can be enhanced in these areas:\n\n- Document extraction and parsing from more diverse formats with higher reliability.\n- Retrieval-augmented analysis (RAG) to provide relevant, contextualized responses driven by resume/job posting content.\n- Multi-step analysis workflows (e.g., extract, match, gap, suggest, generate cover letter) that are easier to reason about and test.\n- Conversation/history-driven improvements for subsequent user flows (cover letters, follow-ups) using memory.\n- Tool-based enrichment for external data lookups (job market trends, LinkedIn company info, skill taxonomies).\n\nThe goal is to introduce LangChain as an internal building block while preserving the external API and interfaces for the rest of the system.\n\n## Requirements\n\nFunctional requirements:\n\n1. Add support for LangChainJS as an optional service implementation for analysis flows.\n2. Provide RAG capabilities for skill matching and gap analysis.\n3. Implement document loaders for both job postings and resume files with better quality extraction, and chunking for embeddings.\n4. Add memory and conversation history to improve subsequent requests (cover letters, iterative analysis).\n5. Create reusable chains for common tasks (skill extraction, match scoring, gap identification, suggestion generation).\n6. Add optional tool integration for external data (job market data, skill taxonomies) behind a configuration flag.\n7. Maintain existing LLMServiceInterface so switching is backward compatible and incremental.\n\nNon-functional requirements:\n\n- Incremental rollout with minimal risk to production flows.\n- Keep cost controls and rate limiting for embeddings and LLM calls.\n- Maintain test coverage and minimal interface changes.\n- No large infra shifts (start with in-memory vector stores for PoC; production-grade vector stores optional later).\n\n## Technical Approach\n\n1. Use LangChainJS (the Node/TypeScript variant) for local integration to match the current stack.\n2. Build a new service &#96;LangChainLLMService&#96; implementing &#96;LLMServiceInterface&#96;. This will be the entrypoint for analysis that leverages chains, memory, and vector stores.\n3. Create document loader utilities for the common resume and posting types (PDF, TXT, MD). Use LangChain document loaders and add a small wrapper to integrate with the project's file storage/repository pattern.\n4. Add &#96;MemoryVectorStore&#96; for PoC RAG; plan migration to a production vector store (Pinecone, Milvus, or Weaviate) for scale.\n5. Implement these initial chains:\n   - &#96;skillExtractionChain&#96;: Extract a canonical list of skills and fuzzy synonyms from a resume or job posting.\n   - &#96;vectorizationChain&#96;: Build embeddings and populate vector store.\n   - &#96;ragAnalysisChain&#96;: Given job posting content and vector store, retrieve top-k resume/context sections to produce analysis and match scores.\n   - &#96;gapAnalysisChain&#96;: Use the retrieved context to produce suggested improvements.\n6. Add &#96;ConversationMemory&#96; integration to store per-user or per-resume analysis history. Use a Date/Version stamp so we can invalidate or refresh memory when underlying content changes.\n7. Provide a &#96;tools&#96; abstraction (LangChain Tools) for external data lookups (job market, skill taxonomy). Use these tools in the agent-based chain for optional features.\n8. Add configuration toggles to the UI and server for enabling or disabling LangChain-based flows, memory, vector store provider selection, and tooling.\n\n## Implementation Steps (Phased)\n\nPhase 0 ‚Äî Preparation\n- Add &#96;langchain&#96; to the project dependencies and any required embedding provider packages (e.g., &#96;@pinecone-database/pinecone&#96; or an OpenAI embedders package if required for PoC).\n- Add basic docs for &#96;LangChainLLMService&#96; and new config options.\n\nPhase 1 ‚Äî PoC: Document Loaders &amp; RAG (2-4 days)\n- Implement &#96;DocumentLoader&#96; utilities integrating LangChain's loaders.\n- Create &#96;MemoryVectorStore&#96; PoC wrapper to store embeddings.\n- Implement a &#96;LangChainLLMService&#96; that performs the analysis by:\n  - Loading the resume / job posting docs\n  - Chunking and embedding\n  - Storing and searching for relevant contexts with the vector store\n  - Running a simple chain for match/gap/suggestion generation.\n- Add minimal unit tests with mock embeddings and LLM services.\n\nPhase 2 ‚Äî Chains, Memory, and Tools (4-8 days)\n- Build modular chains: skill extraction, gap analysis, cover letter generation chain.\n- Add memory (per-resume or per-user) using LangChain's memory, backed by local storage for MVP.\n- Build tools for job market research and skill taxonomy lookup; implement small mock version for testing.\n- Add integration tests that run chains with a mock LLM provider.\n\nPhase 3 ‚Äî UI integration &amp; API (2-4 days)\n- Add a &#96;useLangChain&#96; config flag in &#96;composables&#96; and a switch in the &#96;ServiceSelector&#96; UI.\n- Update &#96;useAnalysis&#96; to call &#96;LangChainLLMService&#96; when enabled.\n- Add toggles in the admin pages for vector store provider selection, memory TTL, and tool options.\n- Add a migration/cleanup flow for vector store content as part of the repo or server-side housekeeping.\n\nPhase 4 ‚Äî Production-ready enhancements &amp; deployment (6-10 days)\n- Add support for a production-grade vector store (Pinecone or Weaviate) as a provider option.\n- Add request-level metrics and usage logging for cost and performance alerts.\n- Add rate limiting and cost controls for embeddings/LLM calls.\n- Extend test coverage, add Playwright E2E tests for the full flow.\n\n## Testing Strategy\n\n- Unit Tests:\n  - Mock the LLM and vector store to test chain logic and extraction flows. Add tests for the new &#96;LangChainLLMService&#96; that confirm calls and output shape.\n  - Validate document loading and chunking logic.\n- Integration Tests:\n  - Use a mock or test LLM provider (e.g., &#96;MockLLMService&#96;) plus in-memory vector store to confirm RAG flows work end-to-end for a variety of resume and job posting inputs.\n  - Test memory persistence and invalidation behavior.\n- E2E Tests:\n  - Playwright tests covering analysis flow with LangChain enabled, cover letter generation, and the admin toggles for disabling/enabling the feature.\n- Performance/Stress Testing:\n  - Benchmark embedding and retrieval latency with representative dataset sizes.\n- Acceptance Criteria:\n  - The &#96;LangChainLLMService&#96; generates the same or better quality analysis than the current system for a standard test set of job postings and resumes.\n  - The UI remains unchanged except for feature toggles and selectors.\n  - The PoC RAG does not produce a significant (&gt; 15%) latency overhead for typical inputs.\n\n## Risks &amp; Mitigation\n\n- Risk: Increased complexity and maintenance overhead.\n  - Mitigation: Start with a small PoC; keep the existing LLM integration untouched and behind a feature flag; incrementally move chains into production.\n- Risk: Cost increases due to embeddings + additional LLM calls.\n  - Mitigation: Add safeguards: sample rate limiting, TTL for vectors, instrumentation &amp; monitoring, local caching for repeated queries.\n- Risk: Latency/performance regressions.\n  - Mitigation: Use in-memory vector store for PoC; choose a production vector provider for scale; batch embeddings and calls where possible.\n- Risk: Security issues (exposing data to vector stores or third-party services).\n  - Mitigation: Use secure env configurations, keep sensitive data on server-side only, allow on-prem vector store providers.\n- Risk: Compatibility &amp; vendor lock-in.\n  - Mitigation: Continue to allow multiple vector store and embedding providers (config-based). Keep &#96;LLMServiceInterface&#96; compatibility.\n\n## Dependencies\n\n- &#96;langchain&#96; (JS/TS) ‚Äî core library for chains, memory, tools\n- Embedding providers (OpenAI embeddings or other provider) for vectorization\n- Optional vector store connectors: Pinecone, Weaviate, Milvus, or an in-house DB (PoC uses &#96;MemoryVectorStore&#96;)\n- Additional packages for PDF/diff parsing if not covered already\n- Dev infra: tests and CI updates for E2E/Playwright and Jest/Vitest\n\n## Acceptance Criteria\n\n- Implement &#96;LangChainLLMService&#96; and wire it as an optional LLM service through the existing &#96;LLMServiceFactory&#96;.\n- Add docs explaining how to enable LangChain and how to configure vector store provider and memory settings.\n- Implement at least one full RAG-based job/resume matching flow in &#96;LangChainLLMService&#96; used for comparisons.\n- Add tests (unit/integration/E2E)\n- Add admin/config UI to choose the provider and feature toggles\n\n## Rollout &amp; Milestones\n\n- Milestone 1 (PoC complete): &#96;LangChainLLMService&#96; PoC with &#96;MemoryVectorStore&#96; and document loaders ‚Äî T+ 2 weeks.\n- Milestone 2 (Feature complete): RAG + chains + memory with tests and UI toggles ‚Äî T+ 4 weeks.\n- Milestone 3 (Production-ready): Integrate production vector store, metrics, and cost controls ‚Äî T+ 8 weeks.\n\n## Next Steps\n\n1. Review this plan and confirm acceptance or propose edits.\n2. Add a PRD or update an existing plan in &#96;.taskmaster/docs/&#96; if a longer initiative is expected.\n3. Create discrete tasks in Taskmaster for each phase (PoC, Chains, UI, Prod) and assign owners.\n\n---\n\nCreated by: Job Posting Analyzer Team\nDate: 2025-11-18\n",
  "toolCallResult": null
}
```
Status: **completed**


</details>
</tool-use>

---

<tool-use data-tool-type="task" data-tool-name="manage_todo_list">
<details>
<summary>Tool use: **manage_todo_list** ‚Ä¢ Using "Manage and track todo items for task planning"</summary>

**Starting: *Create Markdown file* (2/3)**

- [x] Draft plan
  > Create a plan file in docs/plans/ describing LangChain integration, with scope, approach, steps, testing, and risks
- [-] Create Markdown file
  > Write the actual docs/plans/13.langchain-integration.md file with full content
- [ ] Review and finalize
  > Review the plan file for completeness, follow project plan structure and mark as completed

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="copilot_readFile">
<details>
<summary>Tool use: **copilot_readFile** ‚Ä¢ Reading [](file:///Users/michaelpaulukonis/projects/job-posting-analyzer/docs/plans/13.langchain-integration.md)</summary>


Read [](file:///Users/michaelpaulukonis/projects/job-posting-analyzer/docs/plans/13.langchain-integration.md)

</details>
</tool-use>

---

<tool-use data-tool-type="task" data-tool-name="manage_todo_list">
<details>
<summary>Tool use: **manage_todo_list** ‚Ä¢ Using "Manage and track todo items for task planning"</summary>

**Completed: *Create Markdown file* (2/3)**

- [x] Draft plan
  > Create a plan file in docs/plans/ describing LangChain integration, with scope, approach, steps, testing, and risks
- [x] Create Markdown file
  > Write the actual docs/plans/13.langchain-integration.md file with full content
- [x] Review and finalize
  > Review the plan file for completeness, follow project plan structure and mark as completed

</details>
</tool-use>

---

**Planning LangChain Integration**

I need to create a markdown file in `docs/plans/` to explain how LangChain can benefit the project. This process is non-trivial and involves multiple steps, so I must utilize `manage_todo_list` to outline tasks and track progress. The file should adhere to the "Plan-File Requirement" in the `.github/copilot-instructions.md`. I‚Äôll name it "NN.langchain-integration.md" and ensure it includes a problem statement, requirements, technical approach, and more, crafting a clear and structured plan.**Following Project Standards**

I need to adhere to the project standards and follow the naming convention for the markdown files, like ‚ÄúNN.semantic-name.md.‚Äù I‚Äôm considering names like "13.langchain-integration.md," since ‚Äú12.upwork-proposal-generator.md‚Äù already exists. I must prepare the plan with all required fields: Problem Statement, Requirements, Technical Approach, Implementation Steps, Testing Strategy, Risks & Mitigation, Dependencies. I‚Äôll use `manage_todo_list` to create a couple of tasks and document my progress. Finally, I need to create the file and provide a work update!**Organizing Todo List for Plan**

I need to call `manage_todo_list` and outline my tasks. The first todo is to draft the plan outline and content, which I'll mark as in-progress. The second todo is to create the file in `docs/plans/13.langchain-integration.md`, and the third is to review and finish it. I‚Äôll also gather content with key topics like the problem statement, requirements, technical approach for LangChain, implementation steps, a testing strategy, and potential risks. After creating these tasks, I‚Äôll mark them as completed!**Identifying Mitigations and Dependencies**

I need to outline some mitigations like incremental rollout, feature flags, using a mock provider for tests, and monitoring metrics with limits. For dependencies, I'll focus on Node packages such as LangChain, embedding providers, and vector stores, specifically using LangChainJS in our TypeScript project. The current system has LLM services, and I want to enhance it by creating a "LangChainLLMService" that adapts to the LLMServiceInterface. I also need to detail implementation steps with code for document loading, vector storage options, and keep it straightforward by recommending "MemoryVectorStore" for the proof of concept and considering others later.**Creating the Project Timeline**

I want to outline a timeline with recommended phases like "Phase 1 - PoC Document Loaders and RAG," "Phase 2 - Chains and Memory," "Phase 3 - Agents & Tools," "Phase 4 - UI & Integration," and "Phase 5 - Monitoring & Cost Controls." It's essential that the plan includes a "Testing Strategy" covering unit, integration, and functional tests, along with fallback procedures. I‚Äôll also include "Acceptance Criteria." Finally, I'll update the "docs/plans/" directory with a proper overview and implement everything in code. Let's start by using the `manage_todo_list` tool to kick off the plan!Proceeding to create a plan file in `` that documents the LangChain integration proposal and steps. I'll outline the plan, write the markdown file, and update the todo list while doing so.**Finalizing document review**

I still need to finalize the todo list by reviewing the file and ensuring it conforms to the required content. I‚Äôm thinking of adding a note about "PRD parsing" or mentioning "Use TaskMaster MCP" as needed for the project. Additionally, I'll include "Test Data" and "Example prompts." It might be good to add some sample prompt templates. Let's make sure to mark todo 2 as completed and todo 3 as in progress. I should open the file content to check and confirm everything.Now I'm reviewing the plan for completeness and adding a short examples section (prompt examples and next actionable tasks).‚úÖ Done ‚Äî Plan file created and finalized

I created a plan file describing LangChain integration and added it to `` as requested:

- File created: `13.langchain-integration.md`
- Contents: Problem statement, technical approach, phased implementation, testing strategy, risks & mitigations, dependencies, acceptance criteria, and rollout milestones.

Highlights & Next Steps
- The plan is ready for review and can be used as the basis for generating tasks via Taskmaster (PRD parse or manual task creation).
- If you want, I can:
  1. Convert the plan into discrete Taskmaster tasks (PoC, Chains, UI, Prod) and add them to the project.
  2. Start a PoC branch implementing Phase 1 (document loaders + MemoryVectorStore + LangChainLLMService PoC).
  3. Draft code skeletons or a PR with the `LangChainLLMService` and a minimal RAG chain.

If you'd like any changes or additional detail (e.g., prompting examples, recommended import paths, small POC code examples, or how to wire `LangChainLLMService` into `LLMServiceFactory`), tell me which option you prefer and I'll implement it next. üí°

---

